{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph based Music Recommender\n",
    "\n",
    "This general task consists of 6 subtasks (4 are mandatory and 2 are honor).\n",
    "\n",
    "The playbook contains solutions for all tasks (as required by course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the context and read data\n",
    "\n",
    "### Data description\n",
    "\n",
    "There are two data sources for this task. They are DataFrames in parquet format:\n",
    "* user’s playing history (`data/sample264`)\n",
    "* meta data for track or artist (`/data/meta`)\n",
    "\n",
    "#### User's playing history\n",
    "\n",
    "* `trackId` - id of the track\n",
    "* `userId` - id of the user\n",
    "* `artistId` - id of the artist\n",
    "* `timestamp` - timestamp of the moment the user starts listening to a track\n",
    "\n",
    "#### Meta data for track or artist\n",
    "\n",
    "* `type` - record type. Could be \"track\" or \"artist\"\n",
    "* `Name` - title of the track, if the type == \"track\" and the name of the musician or group, if the type == \"artist\"\n",
    "* `Artist` - states for the creator of the track in case the type == \"track\" \n",
    "and for the name of the musician or group in case the type == \"artist\"\n",
    "* `id` - id of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.enableHiveSupport().master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark_session.read.parquet(\"/data/sample264\").cache()\n",
    "meta = spark_session.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, count, row_number, sum as sql_sum\n",
    "\n",
    "\n",
    "def top_n(df, for_col, by_col, n):\n",
    "    \"\"\"\n",
    "    Calculates top-N rows for each value in \n",
    "    {for_col} column by sum of values from {by_col} column.\n",
    "    \n",
    "    :param df: dataframe to filter\n",
    "    :param for_col: string name of column to filter\n",
    "    :param by_col: string name of column to filter by\n",
    "    :param n: a number which indicates how much top rows\n",
    "              select for each value in {for_col}\n",
    "    :return: filtered dataframe\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(for_col).orderBy(col(by_col).desc())\n",
    "    top_n_df = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "    .filter(col(\"row_number\") <= n) \\\n",
    "    .drop(col(\"row_number\"))\n",
    "    \n",
    "    return top_n_df\n",
    "\n",
    "\n",
    "def normalize_weight(df, aggr_col, weight_col=\"weight\"):\n",
    "    \"\"\"\n",
    "    Normalizes values in specified column with formula: \n",
    "      `norm_weight = weight / total_weight`\n",
    "     where:\n",
    "       * weight - value in {weight_col} column\n",
    "       * total_weight - sum of values in {weight_col}\n",
    "                        grouped by values in {aggr_col}\n",
    "    \n",
    "    Adds column `norm_{weight_col}` to the dataframe with \n",
    "    normalized weights.\n",
    "    \n",
    "    :param df: dataframe to modify\n",
    "    :param aggr_col: column name to aggregate weights by\n",
    "    :param weight_col: column name with weights to normalize\n",
    "    :return: a new dataframe with column for normalized weights\n",
    "    \"\"\"\n",
    "    sums_df = df.groupBy(aggr_col).agg(sql_sum(col(weight_col)).alias(\"total_weight\"))\n",
    "    normalized_df = df.join(sums_df, aggr_col) \\\n",
    "        .withColumn(\"norm_\" + weight_col, col(weight_col) / col(\"total_weight\")) \\\n",
    "        .drop(col(\"total_weight\"))\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Build the edges of the type `track-track`\n",
    "\n",
    "Build the edges of the type `track-track`. To do it you will need to count the collaborative similarity between all the tracks: if a user has started listening to track `B` within 7 minutes after starting track `A`, then you should add `1` to the weight of the edge from vertex `A` to vertex `B` (initial weight is equal to 0).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "userId artistId trackId timestamp\n",
    "7        12        1          1534574189\n",
    "7        13        4          1534574289 \n",
    "5        12        1          1534574389 \n",
    "5        13        4          1534594189 \n",
    "6        12        1          1534574489 \n",
    "6        13        4          1534574689 \n",
    "```\n",
    "\n",
    "The track `1` is similar to the track `4` with the weight 2 (before normalization): the user `7` and the user `6` listened these 2 tracks together in the `7 minutes` long window:\n",
    "\n",
    "```\n",
    "userId 7: 1534574289  - 1534574189 = 100 seconds = 1 min 40 seconds < 7 minutes\n",
    "userId 6: 1534574689 - 1534574489 = 200 seconds = 3 min 20 seconds < 7 minutes\n",
    "```\n",
    "\n",
    "Note that the track `4` is similar to the track `1` with the same weight 2.\n",
    "\n",
    "**Tip:** consider joining the graph to itself with the UserId and remove pairs with the same tracks.For each track choose top 50 tracks ordered by weight similar to it and normalize weights of its edges (divide the weight of each edge on a sum of weights of all edges). Use rank() to choose top 40 tracks as is done in the demo.\n",
    "\n",
    "Sort the resulting Data Frame in the descending order by the column norm_weight, and then in the ascending order this time first by `id1`, then by `id2`. Take top 40 rows, select only the columns `id1`, `id2`, and print the columns `id1`, `id2` of the resulting dataframe.\n",
    "\n",
    "**Output example:**\n",
    "\n",
    "```\n",
    "54719\t\t767867\n",
    "54719\t\t767866\n",
    "50787\t\t32767\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import abs as sql_abs, col\n",
    "\n",
    "\n",
    "# Time interval (in seconds) in which two played tracks\n",
    "# considers as similar. Is equals to 7 minutes.\n",
    "TRACKS_SIMILARITY_TIME_WINDOW = 420\n",
    "\n",
    "# Make track-track edges including filtering by time window\n",
    "track_to_track_df = data.alias(\"d1\") \\\n",
    "    .join(data.alias(\"d2\"),\n",
    "          (col(\"d1.userId\") == col(\"d2.userId\")) & \\\n",
    "          (col(\"d1.trackId\") != col(\"d2.trackId\")) & \\\n",
    "          (sql_abs(col(\"d1.timestamp\") - col(\"d2.timestamp\")) <= TRACKS_SIMILARITY_TIME_WINDOW)) \\\n",
    "    .groupBy(col(\"d1.trackId\").alias(\"track1\"), col(\"d2.trackId\").alias(\"track2\")) \\\n",
    "    .count() \\\n",
    "    .select(col(\"track1\"), col(\"track2\"), col(\"count\").alias(\"weight\")) \\\n",
    "    .cache()\n",
    "\n",
    "# Get top 40 of track2 for each track1.\n",
    "tops_track_to_track_df = top_n(track_to_track_df, \"track1\", \"weight\", 40) \\\n",
    "    .orderBy(col(\"weight\").desc(), col(\"track1\"), col(\"track2\"))\n",
    "\n",
    "# Normalize track-track edges weight.\n",
    "track_to_track_normalized_df = normalize_weight(tops_track_to_track_df, \"track1\") \\\n",
    "    .orderBy(col(\"norm_weight\").desc(), col(\"track1\"), col(\"track2\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1_result = track_to_track_normalized_df.select(col(\"track1\"), col(\"track2\")).take(40)\n",
    "# for val in task1_result:\n",
    "#     print(\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Build the edges of the type `user-track`\n",
    "\n",
    "Build the edges of the type `user-track`. Take the amount of times the track was listened by the user as the weight of the edge from the user’s vertex to the track’s vertex.\n",
    "\n",
    "**Tip:** group the dataframe by columns `userId` and `trackId` and use function `count` of DF API.\n",
    "\n",
    "For each user take top-1000 and normalize them.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by `id1`, then by `id2`. Take top 40 rows, select only the columns `id1`, `id2`, and print the columns `id1`, `id2` of the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "```\n",
    "...\n",
    "195 946408\n",
    "215 860111\n",
    "235 897176\n",
    "300 857973\n",
    "321 915545\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Make user-track edges. The weight is how many times user listens to a track\n",
    "user_to_track_df = data.groupBy(col(\"userId\"), col(\"trackId\")) \\\n",
    "    .count() \\\n",
    "    .select(col(\"userId\"), col(\"trackId\"), col(\"count\").alias(\"weight\")) \\\n",
    "    .cache()\n",
    "\n",
    "# Select top 1000 tracks for each user\n",
    "tops_user_to_track_df = top_n(user_to_track_df, \"userId\", \"weight\", 1000) \\\n",
    "    .orderBy(col(\"weight\").desc(), col(\"userId\"), col(\"trackId\")) \\\n",
    "\n",
    "# Normalize user-track edges weight\n",
    "user_to_track_normalized_df = normalize_weight(tops_user_to_track_df, \"userId\", \"weight\") \\\n",
    "    .orderBy(col(\"norm_weight\").desc(), col(\"userId\"), col(\"trackId\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task2_result = user_to_track_normalized_df.select(col(\"userId\"), col(\"trackId\")).take(40)\n",
    "# for val in task2_result:\n",
    "#     print(\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Build the edges of the type `user-artist`\n",
    "\n",
    "Build the edges of the type `user-artist`. Take the amount of times the user has listened to the artist’s tracks as the weight of the edge from the user’s vertex to the artist’s vertex. \n",
    "\n",
    "**Tip:** group the dataframe by the columns userId and trackId and use the function `count` of DF API. For each user take top-100 artists and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column norm_weight, and then in ascending order this time first by `id1`, then by `id2`. Take top 40 rows, select only the columns `id1`, `id2`, and print the columns `id1`, `id2` of the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "```\n",
    "...\n",
    "131 983068\n",
    "195 997265\n",
    "215 991696\n",
    "235 990642\n",
    "288 1000564\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Make user-artist edges. The weight is how many times user listens to an artist\n",
    "user_to_artist_df = data.groupBy(col(\"userId\"), col(\"artistId\")) \\\n",
    "    .count() \\\n",
    "    .select(col(\"userId\"), col(\"artistId\"), col(\"count\").alias(\"weight\")) \\\n",
    "    .cache()\n",
    "\n",
    "# Select top 100 actors for each user\n",
    "tops_user_to_artist_df = top_n(user_to_artist_df, \"userId\", \"weight\", 100) \\\n",
    "    .orderBy(col(\"weight\").desc(), col(\"userId\"), col(\"artistId\")) \\\n",
    "\n",
    "# Normalize user-artist edges weight\n",
    "user_to_artist_normalized_df = normalize_weight(tops_user_to_artist_df, \"userId\", \"weight\") \\\n",
    "    .orderBy(col(\"norm_weight\").desc(), col(\"userId\"), col(\"artistId\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task3_result = user_to_artist_normalized_df.select(col(\"userId\"), col(\"artistId\")).take(40)\n",
    "# for val in task3_result:\n",
    "#     print(\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Build the edges of the type `artist-track`\n",
    "\n",
    "Build the edges of the type `artist-track`. Take the amount of times the track **HAS BEEN** listened by all users as the weight of the edge from the artist’s vertex to the track’s vertex. \n",
    "\n",
    "**Tip:** group the dataframe by the columns `artistId` and `trackId` and use the function `count` of DF API. For each artist take top-100 tracks and normalize weights.\n",
    "\n",
    "Sort the resulting Data Frame in descending order by the column `norm_weight`, and then in ascending order this time first by `id1`, then by `id2`. Take top 40 rows, select only the columns `id1`, `id2`, and print the columns `id1`, “id2” of the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "```\n",
    "...\n",
    "968017 859321\n",
    "968022 852786\n",
    "968034 807671\n",
    "968038 964150\n",
    "968042 835935\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Make artist-track edges. The weight is how many times\n",
    "# an artist's track has been listened by a user\n",
    "artist_to_track_df = data.groupBy(col(\"artistId\"), col(\"trackId\")) \\\n",
    "    .count() \\\n",
    "    .select(col(\"artistId\"), col(\"trackId\"), col(\"count\").alias(\"weight\")) \\\n",
    "    .cache()\n",
    "\n",
    "# Select top 100 tracks for each artist\n",
    "tops_artist_to_track_df = top_n(artist_to_track_df, \"artistId\", \"weight\", 100) \\\n",
    "    .orderBy(col(\"weight\").desc(), col(\"artistId\"), col(\"trackId\"))\n",
    "\n",
    "# Normalize user-artist edges weight\n",
    "artist_to_track_normalized_df = normalize_weight(tops_artist_to_track_df, \"artistId\", \"weight\") \\\n",
    "    .orderBy(col(\"norm_weight\").desc(), col(\"artistId\"), col(\"trackId\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task4_result = artist_to_track_normalized_df.select(col(\"artistId\"), col(\"trackId\")).take(40)\n",
    "# for val in task4_result:\n",
    "#     print(\"%s %s\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\[Honor] Task 5. Find tracks and artists for a user\n",
    "\n",
    "For the user with `userId=776748` find all the tracks and artists connected to him. Use original dataframe not a normalized one. Sort founded items first by artist then by name in ascending order, leave only columns ”Artist” and “Name” and print top-40.\n",
    "\n",
    "Each output line can take one of the following forms:\n",
    "\n",
    "```\n",
    "Artist: <artist-name> <track-name>\n",
    "Artist: <artist-name> Artist: <artist-name>\n",
    "```\n",
    "\n",
    "These two forms help distinguish `user-track` suggestions (as shown in 1) from `user-artist` suggestions (as shown in 2).\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "```\n",
    "...\n",
    "Artist: Blur Artist: Blur\n",
    "Artist: Blur Girls and Boys\n",
    "Artist: Clawfinger Artist: Clawfinger\n",
    "Artist: Clawfinger Nothing Going On\n",
    "Artist: Disturbed Artist: Disturbed\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "TARGET_USER_ID = \"776748\"\n",
    "\n",
    "user_music_history_df = data.filter(col(\"userId\") == TARGET_USER_ID).cache()\n",
    "user_tracks_df = user_music_history_df.select(col(\"trackId\").alias(\"id\")).distinct()\n",
    "user_artist_df = user_music_history_df.select(col(\"artistId\").alias(\"id\")).distinct()\n",
    "\n",
    "user_artists_tracks_df = user_tracks_df.union(user_artist_df) \\\n",
    "    .join(meta, \"id\") \\\n",
    "    .orderBy(col(\"Artist\"), col(\"Name\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task5_result = user_artists_tracks_df.select(col(\"Artist\"), col(\"Name\")).take(40)\n",
    "# for artist, name in task5_result:\n",
    "#     print(artist, name, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\[Honor] Task 6. Build music recomnedations\n",
    "\n",
    "For the user with `userId=776748` print top-40 recommended tracks. Build music recommendations with the algorithm described in the lesson 3 of the fifth week. Initialize coordinates of vector `x_0` corresponding to the user’s vertex and all the vertices from the task 5 with ones and all other coordinates with zeros. \n",
    "\n",
    "Do 5 iterations.\n",
    "\n",
    "Take `alpha = 0.15` and the next balancing functions:\n",
    "  * beta(user, user → artist) =  0.5\n",
    "  * beta(user, user → track) =  0.5\n",
    "  * beta(track, track → track) = 1\n",
    "  * beta(artist, artist → track) = 1\n",
    "\n",
    "You should receive a table with 3 columns: `name`, `artist` and `rank`. Sort the resulting dataframe in descending order by `rank`, select top 40 recommended tracks, select only the columns `name`, `artist` and `rank`, leave 5 digits after the decimal point in `rank` and print the resulting dataframe.\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "```\n",
    "...\n",
    "Prayer Of The Refugee Artist: Rise Against 1.35278102029\n",
    "Eagle Artist: Gotthard 1.21412311013\n",
    "21 Guns Artist: Green Day 1.17301653219\n",
    "Wait And Bleed Artist: Slipknot 0.921552328559\n",
    "Beautiful disaster Artist: 311 0.921552328559\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_USER_ID = \"776748\"\n",
    "\n",
    "# Beta-functions\n",
    "BETA_USER_ARTIST = 0.5\n",
    "BETA_USER_TRACK = 0.5\n",
    "BETA_TRACK_TRACK = 1.0\n",
    "BETA_ARTIST_TRACK = 1.0\n",
    "\n",
    "# Constants\n",
    "ALPHA = 0.15\n",
    "\n",
    "ITERATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "# Build vertices dataframe with columns:\n",
    "#   * prob - 1.0 if item belongs to user, 0.0 otherwise\n",
    "#   * id - id of an item\n",
    "user_weighted_music_history_df = data \\\n",
    "    .withColumn(\"vprob\", when(col(\"userId\") == TARGET_USER_ID, 1.0).otherwise(0.0)).cache()\n",
    "\n",
    "user_init_vertices = user_weighted_music_history_df \\\n",
    "    .select(col(\"userId\").alias(\"id\"), col(\"vprob\")) \\\n",
    "    .distinct()\n",
    "\n",
    "vertices_df = user_init_vertices \\\n",
    "    .union(user_weighted_music_history_df.select(col(\"trackId\").alias(\"id\"), col(\"vprob\"))) \\\n",
    "    .union(user_weighted_music_history_df.select(col(\"artistId\").alias(\"id\"), col(\"vprob\"))) \\\n",
    "    .distinct() \\\n",
    "    .cache()\n",
    "\n",
    "# Build the initial vertex dataframe\n",
    "u_df = vertices_df.withColumn(\"user_prob\", when(col(\"id\") == TARGET_USER_ID, 1.0).otherwise(0.0)) \\\n",
    "    .select(col(\"id\"), col(\"user_prob\").alias(\"uprob\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Build edges dataframe with columns:\n",
    "#   * src - source vertex id\n",
    "#   * dst - target vertex id\n",
    "#   * prob - probability to go from src to dst with applied beta-function\n",
    "balanced_track_to_track_df = track_to_track_normalized_df \\\n",
    "    .withColumn(\"eprob\", col(\"norm_weight\") * BETA_TRACK_TRACK) \\\n",
    "    .select(col(\"track1\").alias(\"src\"), col(\"track2\").alias(\"dst\"), col(\"eprob\")).cache()\n",
    "\n",
    "balanced_user_to_track_df = user_to_track_normalized_df \\\n",
    "    .withColumn(\"eprob\", col(\"norm_weight\") * BETA_USER_TRACK) \\\n",
    "    .select(col(\"userId\").alias(\"src\"), col(\"trackId\").alias(\"dst\"), col(\"eprob\")).cache()\n",
    "\n",
    "balanced_user_to_artist_df = user_to_artist_normalized_df \\\n",
    "    .withColumn(\"eprob\", col(\"norm_weight\") * BETA_USER_ARTIST) \\\n",
    "    .select(col(\"userId\").alias(\"src\"), col(\"artistId\").alias(\"dst\"), col(\"eprob\")).cache()\n",
    "\n",
    "balanced_artist_to_track_df = artist_to_track_normalized_df \\\n",
    "    .withColumn(\"eprob\", col(\"norm_weight\") * BETA_ARTIST_TRACK) \\\n",
    "    .select(col(\"artistId\").alias(\"src\"), col(\"trackId\").alias(\"dst\"), col(\"eprob\")).cache()\n",
    "\n",
    "balanced_edges_df = balanced_track_to_track_df \\\n",
    "    .union(balanced_user_to_track_df) \\\n",
    "    .union(balanced_user_to_artist_df) \\\n",
    "    .union(balanced_artist_to_track_df) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as sql_sum\n",
    "\n",
    "\n",
    "# Calculate recommendations\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    # Calculate probability to be in vertex v\n",
    "    next_v_probs = vertices_df \\\n",
    "        .join(balanced_edges_df, col(\"id\") == col(\"src\"), \"left\") \\\n",
    "        .fillna({\"eprob\": 0.0}) \\\n",
    "        .withColumn(\"new_prob\", col(\"vprob\") * col(\"eprob\")) \\\n",
    "        .groupBy(\"dst\").agg(sql_sum(col(\"new_prob\")).alias(\"sigma\"))\n",
    "\n",
    "    # Update verticies probabilities\n",
    "    vertices_df = u_df \\\n",
    "        .join(next_v_probs, col(\"id\") == col(\"dst\"), \"left\") \\\n",
    "        .fillna({\"sigma\": 0.0}) \\\n",
    "        .withColumn(\"next_value\", ALPHA * col(\"uprob\") + (1 - ALPHA) * col(\"sigma\")) \\\n",
    "        .select(col(\"id\"), col(\"next_value\").alias(\"vprob\")) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round as sql_round\n",
    "\n",
    "# Retrieve result\n",
    "recommendations_df = vertices_df.filter(col(\"id\") != TARGET_USER_ID) \\\n",
    "    .join(meta, \"id\") \\\n",
    "    .orderBy(col(\"vprob\").desc()) \\\n",
    "    .select(col(\"Name\"), col(\"Artist\"), sql_round(col(\"vprob\"), 5).alias(\"Probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill The DJ Artist: Green Day 1.42809\n",
      "Come Out and Play Artist: The Offspring 1.37473\n",
      "I Hate Everything About You Artist: Three Days Grace 1.37362\n",
      "Prayer Of The Refugee Artist: Rise Against 1.35278\n",
      "Eagle Artist: Gotthard 1.21412\n",
      "21 Guns Artist: Green Day 1.17302\n",
      "Beautiful disaster Artist: 311 0.92155\n",
      "Wait And Bleed Artist: Slipknot 0.92155\n",
      "Here To Stay Artist: Korn 0.91653\n",
      "Hard Rock Hallelujah Artist: Lordi 0.91653\n",
      "Nothing Going On Artist: Clawfinger 0.80983\n",
      "Numb Artist: Linkin Park 0.80292\n",
      "In The End Artist: Linkin Park 0.80292\n",
      "Kryptonite Artist: 3 Doors Down 0.68799\n",
      "Sky is Over Artist: Serj Tankian 0.68799\n",
      "Take It Out On Me Artist: Thousand Foot Krutch 0.47024\n",
      "Girls and Boys Artist: Blur 0.40245\n",
      "Cocaine Artist: Nomy 0.20893\n",
      "Getting Away With Murder Artist: Papa Roach 0.20648\n",
      "Artist: Green Day Artist: Green Day 0.01181\n",
      "Artist: Linkin Park Artist: Linkin Park 0.00472\n",
      "Artist: The Offspring Artist: The Offspring 0.00472\n",
      "Artist: Clawfinger Artist: Clawfinger 0.00472\n",
      "She Keeps Me Up Artist: Nickelback 0.00437\n",
      "The Vengeful One Artist: Disturbed 0.00437\n",
      "Sunday Artist: Iggy Pop 0.00437\n",
      "Artist: Blur Artist: Blur 0.00236\n",
      "Artist: 311 Artist: 311 0.00236\n",
      "Artist: Gotthard Artist: Gotthard 0.00236\n",
      "Artist: Korn Artist: Korn 0.00236\n",
      "Artist: Thousand Foot Krutch Artist: Thousand Foot Krutch 0.00236\n",
      "Artist: Papa Roach Artist: Papa Roach 0.00236\n",
      "Artist: Three Days Grace Artist: Three Days Grace 0.00236\n",
      "Artist: Nomy Artist: Nomy 0.00236\n",
      "Artist: Rise Against Artist: Rise Against 0.00236\n",
      "Artist: Nickelback Artist: Nickelback 0.00236\n",
      "Artist: Serj Tankian Artist: Serj Tankian 0.00236\n",
      "Artist: Disturbed Artist: Disturbed 0.00236\n",
      "Artist: 3 Doors Down Artist: 3 Doors Down 0.00236\n",
      "Artist: Lordi Artist: Lordi 0.00236\n"
     ]
    }
   ],
   "source": [
    "task6_result = recommendations_df.take(40)\n",
    "for name, artist, prob in task6_result:\n",
    "    print(name, artist, prob, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}