{
 "cells": [
  {
   "source": [
    "# \\[Honour Task] Spark assignment 2: Collocations\n",
    "\n",
    "As for the second part of the assignment, your task is to extract collocations: that is word combinations that occur together. For example, \"high school\" or \"roman empire\".\n",
    "\n",
    "To find collocations, you will use NPMI (normalized pointwise mutual information) metric.\n",
    "\n",
    "PMI of two words, a & b, is defined as `PMI(a, b) = ln (P(ab) / (P(a) * P(b))`, where `P(ab)` is the probability of two words coming one after the other, and `P(a)` and `P(b)` are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is `P(a) = # of occurrences of word a / total number of words`, and `P(ab) = # of occurrences of words ‘a b’ / total number of word pairs`.\n",
    "\n",
    "Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "NPMI is computed as `NPMI(a, b) = PMI(a, b) / -ln P(ab)`. This normalizes the quantity to be within the range `[-1; 1]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 1. Create SparkContext."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"yarn\"))\n",
    "\n",
    "# For local run uncomment the lines below.\n",
    "# sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\"))\n",
    "# sc.uiWebUrl\n"
   ]
  },
  {
   "source": [
    "### Step 2. Load and parse data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to files\n",
    "# Local run\n",
    "# STOP_WORDS_FILE = \"stop_words_en.txt\"\n",
    "# DATA_FILE = \"test_data.txt\"\n",
    "\n",
    "# Remote run\n",
    "STOP_WORDS_FILE = \"/datasets/stop_words_en.txt\"\n",
    "DATA_FILE = \"/data/wiki/en_articles_part/articles-part\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a broadcast variable to share stop words list\n",
    "stop_words = []\n",
    "with open(STOP_WORDS_FILE, 'r', encoding='utf-8') as f:\n",
    "    stop_words = {w.strip().lower() for w in f}\n",
    "\n",
    "stop_words_broadcast = sc.broadcast(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = line.rstrip().split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "\n",
    "        return [w.lower() for w in words if w.lower() not in stop_words_broadcast.value]\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "\n",
    "wiki = sc.textFile(DATA_FILE, 4).map(parse_article).cache()\n"
   ]
  },
  {
   "source": [
    "### Step 3. Define main logic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import log\n",
    "\n",
    "\n",
    "def make_pairs(data):\n",
    "    \"\"\"\n",
    "    Makes a pairs of words starting with specified word.\n",
    "    \"\"\"\n",
    "    pairs = [\"%s_%s\" % (data[i], data[i + 1]) for i in range(0, len(data) - 1)]\n",
    "    counter = Counter(pairs)\n",
    "    return [(w, c) for w, c in counter.items()]\n",
    "\n",
    "def npmi(colllocation):\n",
    "    \"\"\"\n",
    "    Calculates NPMI for the collocation.\n",
    "    \"\"\"\n",
    "    colc, colc_count = colllocation\n",
    "    first, second = colc.split(\"_\", 1)\n",
    "    prob_first = words_counts_map.value[first] / float(total_words.value)\n",
    "    prob_second = words_counts_map.value[second] / float(total_words.value)\n",
    "    prob_colc = colc_count / float(total_pairs.value)\n",
    "\n",
    "    pmi = log(prob_colc / (prob_first * prob_second))\n",
    "    npmi = pmi / (-1.0 * log(prob_colc))\n",
    "\n",
    "    return colc, npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate each word count\n",
    "words = wiki.flatMap(lambda x: [(w, 1) for w in x]).reduceByKey(lambda a,b: a + b)\n",
    "words_counts_map = sc.broadcast(words.collectAsMap())\n",
    "\n",
    "# Calculate pairs\n",
    "pairs = wiki.map(lambda x: [el.lower() for el in x]).flatMap(make_pairs).reduceByKey(lambda a,b: a + b)\n",
    "\n",
    "# Calculate total number of words and pairs\n",
    "total_words = words.map(lambda x : x[1]).sum()\n",
    "total_pairs = pairs.map(lambda x: x[1]).sum()\n",
    "\n",
    "total_words = sc.broadcast(total_words)\n",
    "total_pairs = sc.broadcast(total_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate npmi-s\n",
    "npmis = pairs.filter(lambda x: x[1] >= 500).map(npmi)"
   ]
  },
  {
   "source": [
    "### Step 4. Print result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npmi in npmis.top(39, key=lambda x: x[1]):\n",
    "    print(npmi[0])"
   ]
  },
  {
   "source": [
    "### Step 5. Stop Spark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}