{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming assignment 1: Words Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your own WordCount program and process Wikipedia dump. Use the second job to sort  words by quantity in the reverse order (most popular first). Output format:\n",
    "\n",
    "`word <tab> count`\n",
    "\n",
    "The result is the 7th word by popularity and its quantity.\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "`is  126420`\n",
    "\n",
    "> **Hint:** it is possible to use exactly one reducer in the second job to obtain a totally ordered result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Create mapper and reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper_wiki_parser.py\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def log(message, **kwargs):\n",
    "    \"\"\"\n",
    "    Prints a given message to sys.stderr stream.\n",
    "    \"\"\"\n",
    "    print(message, file=sys.stderr, **kwargs)\n",
    "    \n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    counter = Counter(words)\n",
    "    for word, count in counter.items():\n",
    "        log(\"reporter:counter:Wiki stats,Total words,%d\" % 1)\n",
    "        print(\"%s\\t%d\" % (word.lower(), count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer_wiki_parser.py\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "current_word = None\n",
    "word_count = 0\n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    if current_word != word:\n",
    "        if current_word:\n",
    "            print(\"%s\\t%d\" % (current_word, word_count))\n",
    "        \n",
    "        word_count = 0\n",
    "        current_word = word\n",
    "    \n",
    "    word_count += int(count)\n",
    "    \n",
    "if current_word:\n",
    "    print(\"%s\\t%d\" % (current_word, word_count))"
   ]
  },
  {
   "source": [
    "## Step 2. Run MapReduce jobs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first job just count words.\n",
    "\n",
    "The second job uses MapReduce comparator to sort result of the first job by value (count for each word)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "INPUT=\"/data/wiki/en_articles_part\"\n",
    "OUT_DIR=\"coursera_mr_task1\"\n",
    "NUM_REDUCERS=4\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "# Count words\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Word Rating (Count)\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper_wiki_parser.py,reducer_wiki_parser.py \\\n",
    "    -mapper \"python3 mapper_wiki_parser.py\" \\\n",
    "    -reducer \"python3 reducer_wiki_parser.py\" \\\n",
    "    -input ${INPUT} \\\n",
    "    -output ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/summarize > /dev/null\n",
    "\n",
    "# Sort counts\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Word Rating (Summarize)\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2nr \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files reducer_wiki_parser.py \\\n",
    "    -mapper \"cat\" \\\n",
    "    -reducer \"cat\" \\\n",
    "    -input ${OUT_DIR}/count \\\n",
    "    -output ${OUT_DIR}/summarize > /dev/null\n",
    "\n",
    "# Code for obtaining the results\n",
    "hdfs dfs -cat ${OUT_DIR}/summarize/part-00000 | sed -n '7p;8q'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}