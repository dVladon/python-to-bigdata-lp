{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Real-World Applications: TF-IDF\n",
    "\n",
    "In this task Hadoop Streaming is used to process Wikipedia articles dump (/data/wiki/en_articles_part).\n",
    "\n",
    "The purpose of this task is to calculate tf*idf for each pair (word, article) from the Wikipedia dump. Apply the stop words filter to speed up calculations. Term frequency (tf) is a function depending on a term (word) and a document (article):\n",
    "\n",
    "`tf(term, doc_id) = Nt/N`,\n",
    "\n",
    "where `Nt` - quantity of particular term in the document, `N` - the total number of terms in the document (without stop words)\n",
    "\n",
    "Inverse document frequency (idf) is a function depends on a term:\n",
    "\n",
    "`idf(term) = 1/log(1 + Dt)`,\n",
    "\n",
    "where `Dt` - number of documents in the dataset with the particular term.\n",
    "\n",
    "Dataset location: */data/wiki/en_articles_part*\n",
    "\n",
    "Stop words list is in `/datasets/stop_words_en.txt` file.\n",
    "\n",
    "**Format:** `article_id <tab> article_text`\n",
    "\n",
    "**Output:** `tf*idf` for term=’labor’ and article_id=12\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "```\n",
    "0.000351\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"\n",
    "    Reads a file with stop words and parses it to set.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    \n",
    "    with open('stop_words_en.txt', 'r', encoding='utf-8') as f:\n",
    "        words = {w.strip().lower() for w in f}\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    words = [w.lower() for w in re.split(r\"\\W*\\s+\\W*\", text, flags=re.UNICODE) if w.lower() not in stop_words]\n",
    "    total_words = len(words)\n",
    "    counter = Counter(words)\n",
    "    for word, count in counter.items():\n",
    "        tf = float(count) / float(total_words)\n",
    "        print(\"%s\\t%s\\t%f\" % (word, article_id, tf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer_articles_with_word.py\n",
    "\"\"\"\n",
    "This is a reducer function: \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "current_word = None\n",
    "articles_count = 0\n",
    "    \n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, article_id, tf = line.strip().split('\\t', 2)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    if current_word != word:\n",
    "        if current_word:\n",
    "            print(\"%s\\t%d\\t%d\" % (current_word, 0, articles_count))\n",
    "\n",
    "        articles_count = 0\n",
    "        current_word = word\n",
    "\n",
    "    print(\"%s\\t%s\\t%f\" % (word, article_id, float(tf)))\n",
    "    articles_count += 1\n",
    "\n",
    "if current_word:\n",
    "    print(\"%s\\t%d\\t%d\" % (current_word, 0, articles_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer_tfidf.py\n",
    "\"\"\"\n",
    "This is a reducer function: \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "def tfidf(tf, dt):\n",
    "    \"\"\"\n",
    "    Calculates tf*idf for word and article\n",
    "    \"\"\"\n",
    "    idf = 1.0 / (log(1 + dt))\n",
    "    tfidf = tf * idf\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "articles_count = 0\n",
    "    \n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, article_id, tf = line.strip().split('\\t', 2)\n",
    "        article_id, tf = int(article_id), float(tf)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    if article_id == 0:\n",
    "        articles_count = tf\n",
    "    else:\n",
    "        print(\"%s\\t%s\\t%f\" % (word, article_id, tfidf(tf, articles_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "INPUT=\"/data/wiki/en_articles_part\"\n",
    "OUT_DIR=\"coursera_tfidf\"\n",
    "NUM_REDUCERS=4\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "# Count words and tf. Counts articles with particular word.\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"TF-IDF (Count)\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper.py,reducer_articles_with_word.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer_articles_with_word.py\" \\\n",
    "    -input ${INPUT} \\\n",
    "    -output ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/tfidf > /dev/null\n",
    "\n",
    "# Calculate tf-idf\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"TF-IDF (Calculate)\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1 -k2,2n\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -file reducer_tfidf.py \\\n",
    "    -mapper \"cat\" \\\n",
    "    -reducer \"python3 reducer_tfidf.py\" \\\n",
    "    -input ${OUT_DIR}/count \\\n",
    "    -output ${OUT_DIR}/tfidf > /dev/null\n",
    "\n",
    "# Print TF-IDF for word 'labor' and article with id 12\n",
    "hdfs dfs -cat ${OUT_DIR}/tfidf/part-00000 | grep \"labor\t12\" | head -1 | cut -f 3"
   ]
  }
 ]
}