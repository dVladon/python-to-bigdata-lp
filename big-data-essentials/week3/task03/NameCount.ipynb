{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Honour Task] Hadoop Streaming assignment 3: Name Count\n",
    "\n",
    "Make WordCount program for all the names in the dataset. Name is a word with the following properties:\n",
    "* The first character is not a digit (other characters can be digits).\n",
    "* The first character is uppercase, all the other characters that are letters are lowercase.\n",
    "* There are less than 0.5% occurrences of this word, when this word regardless to its case appears in the dataset and  the condition (2) is not met.\n",
    "\n",
    "Order by quantity, most popular first, output format:\n",
    "\n",
    "`name <tab> count`\n",
    "\n",
    "The result is the 5th line in the output.\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "`french 5742`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\"\"\"\n",
    "This is a map function: \n",
    "  map: (<article_id> <text>) -> [(<word> <1 if as name, 0 otherwise>),]\n",
    "\n",
    "Skips all words start with a digital.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def is_name(word):\n",
    "    \"\"\"\n",
    "    Checks is the word used as name. \n",
    "    Condition - the first character is uppercase, \n",
    "    all the other characters that are letters are lowercase.\n",
    "    \n",
    "    :return: 1 if the word used as name, otherwise 0.\n",
    "    \"\"\"\n",
    "    if word[0].isupper() and (len(word) < 2 or word[1:].islower()):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    words = re.split(r\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    for word in words:\n",
    "        if len(word) > 0 and word[0].isalpha():\n",
    "            if is_name(word):\n",
    "                print(\"%s\\t%d\" % (word.lower(), 1))\n",
    "            else:\n",
    "                print(\"%s\\t%d\" % (word.lower(), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\"\"\"\n",
    "This is a reducer function: \n",
    "  combiner: (<word> <1 if as name, 0 otherwise>) -> [(<word> <num as name>]\n",
    "  \n",
    "  A word is a name if there are less than 0.5% occurrences of this word, \n",
    "  when this word regardless to its case appears.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def is_name(total, as_name):\n",
    "    \"\"\"\n",
    "    Returns True if there are less than 0.5% \n",
    "    occurrences of this word when the word is not a name.\n",
    "    \"\"\"\n",
    "    if as_name / float(total) >= 0.95:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "current_word = None\n",
    "word_total_count = 0\n",
    "word_count_as_name = 0\n",
    "    \n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, occurrence = line.strip().split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    if current_word != word:\n",
    "        if current_word and is_name(word_total_count, word_count_as_name):\n",
    "            print(\"%s\\t%d\" % (current_word, word_count_as_name))\n",
    "\n",
    "        word_total_count = 0\n",
    "        word_count_as_name = 0\n",
    "        current_word = word\n",
    "\n",
    "    word_count_as_name += int(occurrence)\n",
    "    word_total_count += 1\n",
    "\n",
    "if current_word and is_name(word_total_count, word_count_as_name):\n",
    "    print(\"%s\\t%d\" % (current_word, word_count_as_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run MapReduce jobs.\n",
    "\n",
    "The first job calculates occurences of each 'name' in the dataset.\n",
    "\n",
    "The second job uses MapReduce comparator to sort result of the first job by value (count for each word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "INPUT=\"/data/wiki/en_articles_part\"\n",
    "OUT_DIR=\"coursera_mr_task3\"\n",
    "NUM_REDUCERS=4\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "# Count words\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Name Count (Count)\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input ${INPUT} \\\n",
    "    -output ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/summarize > /dev/null\n",
    "\n",
    "# Sort counts\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Name Count (Summarize)\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2nr \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -mapper \"cat\" \\\n",
    "    -reducer \"cat\" \\\n",
    "    -input ${OUT_DIR}/count \\\n",
    "    -output ${OUT_DIR}/summarize > /dev/null\n",
    "\n",
    "# Code for obtaining the results\n",
    "hdfs dfs -cat ${OUT_DIR}/summarize/part-00000 | head -6 | tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}