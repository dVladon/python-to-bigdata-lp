{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[Honor Task] Hadoop Streaming assignment 4: Word Groups\n",
    "\n",
    "Calculate statistics for groups of words which are equal up to permutations of letters. For example, `emit`, `item` and `time` are the same words up to a permutation of letters. Determine such groups of words and sum all their counts. Apply stop words filter. Filter out groups that consist of only one word.\n",
    "\n",
    "**Output:** count of occurrences for the group of words, number of unique words in the group, comma-separated list of the words in the group in lexicographical order:\n",
    "\n",
    "`sum <tab> group size <tab> word1,word2,...`\n",
    "\n",
    "**Example:** assume ‘emit’ occurred 3 times, 'item' -- 2 times, 'time' -- 5 times; 3 + 2 + 5 = 10, group contains 3 words, so for this group result is:\n",
    "\n",
    "`10 3 emit,item,time`\n",
    "\n",
    "The result of the task is the output line with word `english`.\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "`7823    eghilns 5   english,helsing,hesling,shengli,shingle`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description.\n",
    "\n",
    "The MapReduce schema for this task:\n",
    "* `map`: `(<article> <text>) -> [(<permutation> <word> <count1>),]`\n",
    "* `combine`: `(<permutation> <word> <count1>) -> [(<permutation> <word> <count2>),]`\n",
    "* `reduce`: `(<permutation> [(<word> <count2>)]) -> [(<total_count> <unique_words_count> [<word>,]),]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\"\"\"\n",
    "This is a map function: \n",
    "  map: (<article> <text>) -> [(<permutation> <word> <count1>),]\n",
    "\n",
    "Calculates all occurrences of a word in an article.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"\n",
    "    Reads a file with stop words and parses it to set.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    \n",
    "    with open('stop_words_en.txt', 'r', encoding='utf-8') as f:\n",
    "        words = {w.strip().lower() for w in f}\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    words = [w.lower() for w in re.split(r\"\\W*\\s+\\W*\", text, flags=re.UNICODE) if w.lower not in stop_words]\n",
    "    counter = Counter(words)\n",
    "    for word, count in counter.items():\n",
    "        print(\"\".join(sorted(word)), word, count, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create the combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile combiner.py\n",
    "\"\"\"\n",
    "This is a combiner function: \n",
    "  combine: (<permutation> <word> <count1>) -> [(<permutation> <word> <count2>),]\n",
    "  \n",
    "  Calculates all occurrences of a word in several articles.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "current_word = None\n",
    "current_permutation = None\n",
    "word_count = 0\n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        permutation, word, count = line.strip().split('\\t', 2)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    if current_word != word:\n",
    "        if current_word:\n",
    "            print(current_permutation, current_word, word_count, sep=\"\\t\")\n",
    "\n",
    "        word_count = 0\n",
    "        current_word = word\n",
    "        current_permutation = permutation\n",
    "\n",
    "    word_count += int(count)\n",
    "\n",
    "if current_word:\n",
    "    print(current_permutation, current_word, word_count, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Create the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "\"\"\"\n",
    "This is a reducer function: \n",
    "  reduce: (<permutation> [(<word> <count2>)]) -> [(<total_count> <unique_words_count> [<word>,]),]\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "current_permutation = None\n",
    "words = set()\n",
    "words_count = 0\n",
    "    \n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        permutation, word, count = line.strip().split('\\t', 2)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    if current_permutation != permutation:\n",
    "        if current_permutation and len(words) > 1:\n",
    "            print(words_count, current_permutation, len(words), \",\".join(words), sep=\"\\t\")\n",
    "\n",
    "        words_count = 0\n",
    "        words = set()\n",
    "        current_permutation = permutation\n",
    "\n",
    "    words_count += int(count)\n",
    "    words.add(word)\n",
    "\n",
    "if current_permutation and len(words) > 1:\n",
    "    print(words_count, len(words), \",\".join(sorted(words)), sep=\"\\t\")"
   ]
  },
  {
   "source": [
    "### Step 4. Run MapReduce jobs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "INPUT=\"/data/wiki/en_articles_part\"\n",
    "OUT_DIR=\"coursera_mr_task4\"\n",
    "NUM_REDUCERS=4\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "# Count words groups\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Word Groups (Count)\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1 -k2\" \\\n",
    "    -files mapper.py,reducer.py,combiner.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -combiner \"python3 combiner.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input ${INPUT} \\\n",
    "    -output ${OUT_DIR}/count > /dev/null\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}/reduce > /dev/null\n",
    "\n",
    "# Sort counts\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Word Groups (Reduce)\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -mapper \"cat\" \\\n",
    "    -reducer \"cat\" \\\n",
    "    -input ${OUT_DIR}/count \\\n",
    "    -output ${OUT_DIR}/reduce > /dev/null\n",
    "\n",
    "# Code for obtaining the results\n",
    "hdfs dfs -cat ${OUT_DIR}/reduce/part-00000 | grep -P '(,|\\t)english($|,)' | head -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}