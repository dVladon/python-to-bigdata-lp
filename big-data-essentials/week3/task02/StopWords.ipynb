{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming assignment 2: Stop Words\n",
    "\n",
    "Improve the previous program to calculate how many stop words are in the input dataset. Stop words list is in `/datasets/stop_words_en.txt` file. Use Hadoop counter to count the number of stop words and total words in the dataset. The result is the percentage of stop words in the entire dataset (without percent symbol).\n",
    "\n",
    "The result on the sample dataset:\n",
    "\n",
    "`41.603`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper_wiki_parser.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def log(message, **kwargs):\n",
    "    \"\"\"\n",
    "    Prints a given message to sys.stderr stream.\n",
    "    \"\"\"\n",
    "    print(message, file=sys.stderr, **kwargs)\n",
    "    \n",
    "    \n",
    "def counter(name, value):\n",
    "    \"\"\"\n",
    "    Prints a MapReduce job counter.\n",
    "    \"\"\"\n",
    "    log(\"reporter:counter:Wiki Stats,%s,%d\" % (name, value))\n",
    "    \n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"\n",
    "    Reads a file with stop words and parses it to set.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    \n",
    "    with open('stop_words_en.txt', 'r', encoding='utf-8') as f:\n",
    "        words = {w.strip().lower() for w in f}\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "\n",
    "# Main block\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = line.strip().split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    words = re.split(r\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            counter(\"Stop words\", 1)\n",
    "        \n",
    "        counter(\"Total words\", 1)\n",
    "        \n",
    "        print(\"%s\\t%d\" % (word.lower(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create the reducer.\n",
    "\n",
    "**Note:** we don't really need a reducer for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Create the parsing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile counter_process.py\n",
    "\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "STOP_WORDS_COUNTER_RE = re.compile(\"Stop words=\\d+\")\n",
    "TOTAL_WORDS_COUNTER_RE = re.compile(\"Total words=\\d+\")\n",
    "\n",
    "\n",
    "def parse_logs():\n",
    "    \"\"\"\n",
    "    Parses raw logs of MapReduce job and \n",
    "    returns values of two counters as tuple.\n",
    "    \"\"\"\n",
    "    stop_words = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        if STOP_WORDS_COUNTER_RE.search(line):\n",
    "            stop_words = int(line.strip().split(\"=\", 1)[1])\n",
    "            \n",
    "        if TOTAL_WORDS_COUNTER_RE.search(line):\n",
    "            total_words = int(line.strip().split(\"=\", 1)[1])\n",
    "    \n",
    "    return stop_words, total_words\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stop_words, total_words = parse_logs()\n",
    "    print(stop_words / float(total_words) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Bash commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "INPUT=\"/data/wiki/en_articles_part\"\n",
    "OUT_DIR=\"coursera_mr_task2\"\n",
    "LOGS=\"stderr_logs.txt\"\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Stop Words\" \\\n",
    "    -D mapreduce.job.reduces=0 \\\n",
    "    -files mapper_wiki_parser.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python3 mapper_wiki_parser.py\" \\\n",
    "    -input ${INPUT} \\\n",
    "    -output ${OUT_DIR} > /dev/null 2> $LOGS\n",
    "    \n",
    "cat $LOGS | python ./counter_process.py\n",
    "cat $LOGS >&2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}