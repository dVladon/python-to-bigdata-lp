{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example of how to work with SparkSQL and DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "                            .appName(\"myApp\") \\\n",
    "                            .master(\"local\") \\\n",
    "                            .enableHiveSupport() \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_rdd = spark_session.sparkContext.textFile('/data/subnets/ips/subnets_var1_len250.txt')\n",
    "ips_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "# Create data scheme\n",
    "data_schema = StructType().add(\"ip\", StringType())\\\n",
    "                          .add(\"mask\", StringType())\n",
    "\n",
    "# Create DataFrame from RDD\n",
    "ips_df = spark_session.createDataFrame(ips_rdd.map(lambda x: x.split(\"\\t\", 1)), data_schema)\n",
    "\n",
    "ips_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple operations with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top N rows\n",
    "ips_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame schema\n",
    "ips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DataFrame to Hive table\n",
    "# Values for mode property:\n",
    "#   * error - throw an error if table already exists\n",
    "#   * overwrite\n",
    "#   * append\n",
    "ips_df.write.saveAsTable(\"default.ips\", mode=\"overwrite\")\n",
    "spark_session.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DataFrame as Parquet file\n",
    "ips_df.write.save(\"ips.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with RDD inside DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with DataFrame' RDD\n",
    "ips_df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Select data\n",
    "# \n",
    "# Return types:\n",
    "#   * select: Spark DataFrame [n columns] -> Spark DataFrame [m columns]\n",
    "#   * where:  Spark DataFrame -> Spark DataFrame\n",
    "#   * show:   NoneType\n",
    "ips_df.select(\"ip\").where(\"mask = '255.255.255.128'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with DataFrame via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view.\n",
    "# The view is temporary because it exists\n",
    "# only during Spark session.\n",
    "ips_df.createTempView(\"ips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute an SQL query\n",
    "select_result = spark_session.sql(\"\"\"SELECT * FROM ips\"\"\")\n",
    "select_result.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show databases and tables\n",
    "spark_session.sql(\"\"\"SHOW DATABASES\"\"\").toPandas()\n",
    "spark_session.sql(\"\"\"SHOW TABLES IN default\"\"\").toPandas()\n",
    "\n",
    "# or \n",
    "spark_session.catalog.listDatabases()\n",
    "spark_session.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a permanent table from view\n",
    "spark_session.sql(\"\"\"CREATE DATABASE IF NOT EXISTS main\"\"\")\n",
    "spark_session.sql(\"\"\"DROP TABLE IF EXISTS main.ips\"\"\")\n",
    "spark_session.sql(\"\"\"\n",
    "    CREATE TABLE main.ips AS\n",
    "    SELECT * FROM ips\n",
    "\"\"\")\n",
    "\n",
    "spark_session.catalog.listTables(\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table from disk\n",
    "ips_from_disk = spark_session.read.table(\"main.ips\")\n",
    "ips_from_disk.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}